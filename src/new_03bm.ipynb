{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4116cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, \n",
    "                          Trainer,\n",
    "                          DataCollatorWithPadding)\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3107b1b",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae02b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = pd.read_csv('geo-reviews-dataset-2023.csv') # –ö–æ–ª–æ–Ω–∫–∏: 'review', 'label'\n",
    "df_1 = df.loc[df['label'] == 1]\n",
    "df_2 = df.loc[df['label'] == 2]\n",
    "df_3 = df.loc[df['label'] == 3]\n",
    "df_4 = df.loc[df['label'] == 4]\n",
    "df_5 = df.loc[df['label'] == 5]\n",
    "\n",
    "df_new = pd.concat([df_1[0:1000], df_2[0:1000], df_3[0:1000], df_4[0:1000], df_5[0:1000]], ignore_index=True)\n",
    "df_new = df_new.sample(frac=1).reset_index(drop=True) #–ø–µ—Ä–µ–º–µ—à–∞—Ç—å\n",
    "#df = df.sort_values(by='label')\n",
    "#df = df[260:4600]\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fb839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–æ–∂—É –≤ –î–µ—Ç—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä \"–ü–ª–∞–Ω–µ—Ç–∞ —Ä–µ–±—ë–Ω–æ–∫\" —Å–≤–æ–µ–≥–æ ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –æ–±–æ–∂–∞—é –≥—Ä—É–∑–∏–Ω—Å–∫—É—é –∫—É—Ö–Ω—é, –Ω–æ –±—ã–ª–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 –∏–∑ 5üñ§ –ü–∏–ª –∫–æ—Ñ–µ –∏ –≤ –†–∏–º–µ, –∏ –≤  –ü–∞—Ä–∏–∂–µ, –Ω–æ –≤–∫—É...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–î–æ—Å–∫–∏ –∫–∞–∫ –¥–æ—Å–∫–∏. –ê–∫—É—Å—Ç–∏–∫–∞ –ø—Ä–∞–≤–¥–∞ –ª—é—Ç–∞—è. –ê —Ç–∞–∫,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–û—á–µ–Ω—å –∫—Ä–∞—Å–∏–≤—ã–π –≤–∏–¥ –∏ –æ—á–µ–Ω—å –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–æ–µ –º–µ—Å—Ç–æ, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  –í–æ–∂—É –≤ –î–µ—Ç—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä \"–ü–ª–∞–Ω–µ—Ç–∞ —Ä–µ–±—ë–Ω–æ–∫\" —Å–≤–æ–µ–≥–æ ...      5\n",
       "1  –Ø –æ–±–æ–∂–∞—é –≥—Ä—É–∑–∏–Ω—Å–∫—É—é –∫—É—Ö–Ω—é, –Ω–æ –±—ã–ª–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω...      2\n",
       "2  5 –∏–∑ 5üñ§ –ü–∏–ª –∫–æ—Ñ–µ –∏ –≤ –†–∏–º–µ, –∏ –≤  –ü–∞—Ä–∏–∂–µ, –Ω–æ –≤–∫—É...      5\n",
       "3  –î–æ—Å–∫–∏ –∫–∞–∫ –¥–æ—Å–∫–∏. –ê–∫—É—Å—Ç–∏–∫–∞ –ø—Ä–∞–≤–¥–∞ –ª—é—Ç–∞—è. –ê —Ç–∞–∫,...      3\n",
       "4  –û—á–µ–Ω—å –∫—Ä–∞—Å–∏–≤—ã–π –≤–∏–¥ –∏ –æ—á–µ–Ω—å –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–æ–µ –º–µ—Å—Ç–æ, ...      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f3f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([df_1[1001:1200], df_2[1001:1200], df_3[1001:1200], df_4[1001:1200], df_5[1001:1200]], ignore_index=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc86a7",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a144b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'–æ–Ω–∏', '—Ç–∞–∫', '–º—ã', '—á—Ç–æ–±', '–ø—Ä–∏', '–æ–±', '–ø–æ—á—Ç–∏', '—á–µ—Ä–µ–∑', '—Ç–µ–±—è', '–ø–æ—Å–ª–µ', '–≤—ã', '—ç—Ç–∏', '–º–Ω–æ–≥–æ', '–∫–æ–Ω–µ—á–Ω–æ', '—Ä–∞–∑–≤–µ', '–±–æ–ª–µ–µ', '–¥–∞–∂–µ', '–Ω–∞–¥–æ', '–Ω–∞–∫–æ–Ω–µ—Ü', '–≤—Å–µ–≥–¥–∞', '–∏', '—á–µ–º', '—ç—Ç–æ—Ç', '—Ç–µ–º', '–æ–¥–∏–Ω', '–Ω–µ–ª—å–∑—è', '–Ω—É', '–∫—Ç–æ', '–±—ã—Ç—å', '—Ö–æ—Ç—å', '–∞', '—Ç–æ', '–≤–¥—Ä—É–≥', '—Ç—Ä–∏', '–±—ã–ª–∏', '–≤–æ', '–∫—É–¥–∞', '—Ç–µ–ø–µ—Ä—å', '—É–∂–µ', '–µ–π', '–ø–æ–¥', '–ø–æ—Ç–æ–º—É', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É', '—Ç–∞–∫–æ–π', '—Ä–∞–∑', '–≤–∞–º', '–±—ã', '–Ω–∏—Ö', '—Ç—É—Ç', '–µ–≥–æ', '—Ç—ã', '—á—É—Ç—å', '—Ç–∞–º', '—Å–≤–æ—é', '—Ç–æ–≥–æ', '–∏—Ö', '—É–∂', '–µ—Å–ª–∏', '–º–æ–∂–Ω–æ', '–ø–æ—Ç–æ–º', '–±—ã–ª–∞', '—Å–æ–≤—Å–µ–º', '–µ—â–µ', '—á–µ–≥–æ', '–±—É–¥—Ç–æ', '—Ç–æ–≥–¥–∞', '—Ç–æ–∂–µ', '—Å–æ', '—Å', '–∫–æ–≥–¥–∞', '–æ–Ω–∞', '–≥–¥–µ', '–∫', '–≤–∞—Å', '–∫–∞–∫–æ–π', '—ç—Ç–æ–π', '–æ–ø—è—Ç—å', '–º–æ–π', '–≤—Å–µ—Ö', '–Ω–æ', '–µ–µ', '–Ω–∏–±—É–¥—å', '–≤—Å—é', '–µ–º—É', '–º–æ–∂–µ—Ç', '–¥–≤–∞', '–∑–¥–µ—Å—å', '–º–µ–Ω—è', '–∏–∑', '–Ω–∏—á–µ–≥–æ', '–∏–ª–∏', '—Å–µ–±–µ', '—Å–µ–π—á–∞—Å', '—è', '—ç—Ç—É', '—Å–∞–º', '–Ω–µ–µ', '–º–æ—è', '–ª—É—á—à–µ', '–¥—Ä—É–≥–æ–π', '–±—É–¥–µ—Ç', '–∂–µ', '–Ω–µ–≥–æ', '–∂', '–≤–æ—Ç', '–±—ã–ª–æ', '–∫–∞–∫', '—Ç–æ–º', '–≤–ø—Ä–æ—á–µ–º', '–≤–µ–¥—å', '—ç—Ç–æ–≥–æ', '–ø–æ', '–¥–ª—è', '–¥–æ', '–∑–∞—á–µ–º', '–æ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '–Ω–µ—Ç', '–Ω–∏', '–ø—Ä–æ', '—Ö–æ—Ä–æ—à–æ', '—á—Ç–æ–±—ã', '–±–µ–∑', '–∏–Ω–æ–≥–¥–∞', '—Ç–æ—Ç', '–∫–∞–∫–∞—è', '–Ω–∏–º', '–µ—Å—Ç—å', '–Ω–µ–π', '–ª–∏', '–¥–∞', '–Ω–∞—Å', '–Ω–∏–∫–æ–≥–¥–∞', '–≤—Å–µ', '–æ–Ω', '–≤', '–∏–º', '—Ç–æ–ª—å–∫–æ', '–º–Ω–µ', '–∑–∞', '—É', '–æ—Ç', '–±—ã–ª', '–Ω–∞', '—ç—Ç–æ–º', '—á—Ç–æ', '–≤—Å–µ–≥–æ', '—Å–µ–±—è', '–Ω–µ'},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û—Å—Ç–∞–≤–∏–º –≤ —Ç–µ–∫—Å—Ç–µ —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã\n",
    "def clear_text(text):\n",
    "    clear_text = re.sub(r'[^–ê-—è–Å—ë]+', ' ', text).lower()\n",
    "    return \" \".join(clear_text.split())\n",
    "\n",
    "# –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é —É–¥–∞–ª—è—é—â—É—é —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "def clean_stop_words(text, stopwords):\n",
    "    text = [word for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "# –∑–∞–≥—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "stopwords = set(nltk_stopwords.words('russian'))\n",
    "np.array(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814cabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–ª–∏—á–Ω–∞—è —Ö–∏–º—á–∏—Å—Ç–∫–∞. –ü–æ—á–∏—Å—Ç–∏–ª–∏ –∑–∏–º–Ω–∏–π –ø—É—Ö–æ–≤–∏–∫ - –∫–∞–∫ –Ω–æ–≤–µ–Ω—å–∫–∏–π —Å—Ç–∞–ª! –û–ø–µ—Ä–µ–∂–∞—é—Ç —Å—Ä–æ–∫–∏ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥. –£–¥–æ–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∫–∏–¥–æ–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–∫–∞–∑—ã. –•–æ—Ä–æ—à–∞—è —É–ø–∞–∫–æ–≤–∫–∞ (–∫–∞–∫ –ø–æ–ª–∏—ç—Ç–∏–ª–µ–Ω–æ–≤—ã–π –∫–æ—Ñ—Ä), —É–ª–æ–≤–Ω—ã–µ –ø–ª–µ—á–∏–∫–∏. –û–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!  –ù–µ –∑–Ω–∞—é –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö –ø—É–Ω–∫—Ç–∞—Ö –ø—Ä–∏–µ–º–∞ —ç—Ç–æ–π —Ö–∏–º—á–∏—Å—Ç–∫–∏, –Ω–æ –≤ –¢–¶ ¬´–ï–≤—Ä–æ–ø–∞-–°–ò–¢–ò¬ª –≤—Å–µ –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ üëç \n",
      "=======================================\n",
      "–æ—Ç–ª–∏—á–Ω–∞—è —Ö–∏–º—á–∏—Å—Ç–∫–∞ –ø–æ—á–∏—Å—Ç–∏–ª–∏ –∑–∏–º–Ω–∏–π –ø—É—Ö–æ–≤–∏–∫ –Ω–æ–≤–µ–Ω—å–∫–∏–π —Å—Ç–∞–ª –æ–ø–µ—Ä–µ–∂–∞—é—Ç —Å—Ä–æ–∫–∏ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥ —É–¥–æ–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∫–∏–¥–æ–∫ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–∫–∞–∑—ã —Ö–æ—Ä–æ—à–∞—è —É–ø–∞–∫–æ–≤–∫–∞ –ø–æ–ª–∏—ç—Ç–∏–ª–µ–Ω–æ–≤—ã–π –∫–æ—Ñ—Ä —É–ª–æ–≤–Ω—ã–µ –ø–ª–µ—á–∏–∫–∏ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∑–Ω–∞—é –¥—Ä—É–≥–∏—Ö –ø—É–Ω–∫—Ç–∞—Ö –ø—Ä–∏–µ–º–∞ —Ö–∏–º—á–∏—Å—Ç–∫–∏ —Ç—Ü –µ–≤—Ä–æ–ø–∞ —Å–∏—Ç–∏ –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "review = df_new['review'][np.random.randint(df_new.shape[0])]\n",
    "print(review)\n",
    "print('=======================================')\n",
    "print(clean_stop_words((clear_text(review)), stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f9c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–Ω—è–ª–∞: 0.16 —Å–µ–∫—É–Ω–¥\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# –û—á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤\n",
    "start_clean = time.time()\n",
    "\n",
    "df_new['review'] = df_new['review']\\\n",
    "     .apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))\n",
    "\n",
    "print('–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–Ω—è–ª–∞: '+str(round(time.time() - start_clean, 2))+' —Å–µ–∫—É–Ω–¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e864ad4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfe95b0482542328661cc85352d14d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "model_name = 'sberbank-ai/ruBert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "# 5. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω—É–∂–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6e25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d14e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_m2',          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    eval_strategy=\"epoch\",     # –û—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    learning_rate=2e-5,              # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    per_device_train_batch_size=16,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "    weight_decay=0.01,               # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71b1660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f73007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_13484\\899701250.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 05:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.070456</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>0.547044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.069200</td>\n",
       "      <td>1.041454</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.545054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.069200</td>\n",
       "      <td>1.096782</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.543654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. –°–æ–∑–¥–∞–µ–º Data Collator –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 9. –°–æ–∑–¥–∞–µ–º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 10. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ!\n",
    "trainer.train()\n",
    "\n",
    "# 11. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "trainer.save_model('./my_sentiment_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e90eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "725cf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('./my_sentiment_model_2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./my_sentiment_model_2')\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è\n",
    "def predict_sentiment(text):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # –ü–æ–¥–∞–µ–º –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –±–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å (—É—Ç–æ—á–Ω–∏—Ç–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫ –º–æ–¥–µ–ª–∏)\n",
    "    classes = ['–ù–ï–ì–ê–¢–ò–í–ù–´–ô', '–ü–û–ó–ò–¢–ò–í–ù–´–ô'] \n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "    predicted_class_idx = predicted.argmax().item()\n",
    "    sentiment = predicted_class_idx #classes[predicted_class_idx]\n",
    "    confidence = predicted[0][predicted_class_idx].item()\n",
    "    \n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä\n",
    "# –û—á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤\n",
    "test_df['review'] = test_df['review']\\\n",
    "     .apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a37ef35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>spread_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –µ–¥–∞ —à–≤–µ–¥—Å–∫–æ–º —Å—Ç–æ–ª–µ —Å—Ç–∞...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—Ç—ë–ø–ª—ã–π –ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ñ–µ–Ω —à–∞–º–ø—É–Ω—å –≥–µ–ª—å –¥—É—à–∞ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ –º–µ—Å—Ç–æ –ª–µ—Ç–æ–º –ø—Ä–∏–µ—Ö–∞—Ç—å –ø–∞–ª–∞—Ç–∫–∞–º–∏ –∫—Ä...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Ç–∏—Ö–æ–µ –º–µ—Å—Ç–µ—á–∫–æ –Ω–µ–¥–∞–ª–µ–∫–æ —Ü–µ–Ω—Ç—Ä–∞—Ç–≥–æ—Ä–æ–¥–∞ –∫—É—Ö–Ω—è –º–∏...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–æ–≤–æ—â–∏ –±–∏—Ç—ã–µ –ø–æ–º–∏–¥–æ—Ä—ã –≥–Ω–∏–ª—ã–µ —á–µ—Å—Ç–Ω–æ–µ –º–µ—Ä–∑–ª—ã–π –∫–∞...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  prediction  \\\n",
       "0  –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –µ–¥–∞ —à–≤–µ–¥—Å–∫–æ–º —Å—Ç–æ–ª–µ —Å—Ç–∞...      3           4   \n",
       "1  —Ç—ë–ø–ª—ã–π –ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ñ–µ–Ω —à–∞–º–ø—É–Ω—å –≥–µ–ª—å –¥—É—à–∞ ...      3           4   \n",
       "2  –≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ –º–µ—Å—Ç–æ –ª–µ—Ç–æ–º –ø—Ä–∏–µ—Ö–∞—Ç—å –ø–∞–ª–∞—Ç–∫–∞–º–∏ –∫—Ä...      5           5   \n",
       "3  —Ç–∏—Ö–æ–µ –º–µ—Å—Ç–µ—á–∫–æ –Ω–µ–¥–∞–ª–µ–∫–æ —Ü–µ–Ω—Ç—Ä–∞—Ç–≥–æ—Ä–æ–¥–∞ –∫—É—Ö–Ω—è –º–∏...      3           4   \n",
       "4  –æ–≤–æ—â–∏ –±–∏—Ç—ã–µ –ø–æ–º–∏–¥–æ—Ä—ã –≥–Ω–∏–ª—ã–µ —á–µ—Å—Ç–Ω–æ–µ –º–µ—Ä–∑–ª—ã–π –∫–∞...      1           1   \n",
       "\n",
       "   spread_distance  \n",
       "0                1  \n",
       "1                1  \n",
       "2                0  \n",
       "3                1  \n",
       "4                0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 –º–∏–Ω—É—Ç–∞ –Ω–∞ 1000 –æ—Ç–∑—ã–≤–æ–≤\n",
    "test_df['prediction'] = test_df['review']\\\n",
    "        .apply(lambda x: predict_sentiment(str(x))[0])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f7ab28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139  36  19   4   1]\n",
      " [ 48  68  56  25   2]\n",
      " [ 22  44  82  46   5]\n",
      " [  2  10  42 109  36]\n",
      " [  2   1   4  26 166]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(test_df['label'], test_df['prediction'])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db4594d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>spread_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –µ–¥–∞ —à–≤–µ–¥—Å–∫–æ–º —Å—Ç–æ–ª–µ —Å—Ç–∞...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—Ç—ë–ø–ª—ã–π –ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ñ–µ–Ω —à–∞–º–ø—É–Ω—å –≥–µ–ª—å –¥—É—à–∞ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ –º–µ—Å—Ç–æ –ª–µ—Ç–æ–º –ø—Ä–∏–µ—Ö–∞—Ç—å –ø–∞–ª–∞—Ç–∫–∞–º–∏ –∫—Ä...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Ç–∏—Ö–æ–µ –º–µ—Å—Ç–µ—á–∫–æ –Ω–µ–¥–∞–ª–µ–∫–æ —Ü–µ–Ω—Ç—Ä–∞—Ç–≥–æ—Ä–æ–¥–∞ –∫—É—Ö–Ω—è –º–∏...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–æ–≤–æ—â–∏ –±–∏—Ç—ã–µ –ø–æ–º–∏–¥–æ—Ä—ã –≥–Ω–∏–ª—ã–µ —á–µ—Å—Ç–Ω–æ–µ –º–µ—Ä–∑–ª—ã–π –∫–∞...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  prediction  \\\n",
       "0  –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –µ–¥–∞ —à–≤–µ–¥—Å–∫–æ–º —Å—Ç–æ–ª–µ —Å—Ç–∞...      3           4   \n",
       "1  —Ç—ë–ø–ª—ã–π –ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ñ–µ–Ω —à–∞–º–ø—É–Ω—å –≥–µ–ª—å –¥—É—à–∞ ...      3           4   \n",
       "2  –≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ –º–µ—Å—Ç–æ –ª–µ—Ç–æ–º –ø—Ä–∏–µ—Ö–∞—Ç—å –ø–∞–ª–∞—Ç–∫–∞–º–∏ –∫—Ä...      5           5   \n",
       "3  —Ç–∏—Ö–æ–µ –º–µ—Å—Ç–µ—á–∫–æ –Ω–µ–¥–∞–ª–µ–∫–æ —Ü–µ–Ω—Ç—Ä–∞—Ç–≥–æ—Ä–æ–¥–∞ –∫—É—Ö–Ω—è –º–∏...      3           4   \n",
       "4  –æ–≤–æ—â–∏ –±–∏—Ç—ã–µ –ø–æ–º–∏–¥–æ—Ä—ã –≥–Ω–∏–ª—ã–µ —á–µ—Å—Ç–Ω–æ–µ –º–µ—Ä–∑–ª—ã–π –∫–∞...      1           1   \n",
       "\n",
       "   spread_distance  \n",
       "0                1  \n",
       "1                1  \n",
       "2                0  \n",
       "3                1  \n",
       "4                0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['spread_distance'] = ((test_df['label'] - test_df['prediction']) ** 2)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5d7eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spread_distance\n",
      "0     564\n",
      "1     334\n",
      "4      85\n",
      "9       9\n",
      "16      3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dups = test_df.pivot_table(index = ['spread_distance'], aggfunc ='size')\n",
    "print(dups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
